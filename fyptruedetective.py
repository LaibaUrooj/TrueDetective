# -*- coding: utf-8 -*-
"""FypTrueDetective.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Nhh4zhmbxvDFuV14NhB8j_dDyL1E2sib
"""

# google colab. 

from google.colab import drive
drive.mount('/content/gdrive/')

"""# **( 1.0 ) NECESSARY LIBRARIES & INSTALLATIONS**"""

#Necessary Installations

!pip install --upgrade tf_slim #*

!sudo pip install mtcnn

# confirm mtcnn was installed correctly
import mtcnn
# print version
print(mtcnn.__version__)

# Commented out IPython magic to ensure Python compatibility.
# Necessary Libraries.

from PIL import Image, ImageOps 
from matplotlib import pyplot
from google.colab.patches import cv2_imshow
from matplotlib.patches import Rectangle
import os.path 
from os import listdir
from keras.models import load_model
import numpy as np 
import tensorflow as tf
from numpy import asarray
import matplotlib as mp
# %matplotlib inline
import matplotlib.pyplot as plt
import skimage
import scipy
import tf_slim as slim # tF-Slim is a library that makes defining, training and evaluating neural networks simple:
#from tensorflow.python.compiler.tensorrt import trt_convert as trt
import math
from PIL import Image
from scipy.ndimage import rotate
from os import mkdir
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import seaborn as sns
from scipy import stats

import warnings
warnings.filterwarnings('ignore')
from sklearn.metrics import confusion_matrix
from keras.models import Sequential, model_from_json
from keras.layers import Dense, Conv2D, Activation, MaxPool2D, Flatten, Dropout, BatchNormalization
from keras.optimizers import RMSprop,Adam
from keras.preprocessing import image
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ModelCheckpoint

"""# **( 2.0 ) FACE DETECTION USING MTCNN**

**( 2.1 ) EXTRACT FACE FUNC** - , 



> **PREPROCESSIGN DATA**


*   Process a single frame
*   Extracting Faces From a single Frame
*   Resize & Bound boxes arround it
"""

# extract a single face from a given photograph

def extract_face(filename, detector, required_size=(96, 96)):
  # load image from file.
	img = Image.open(filename) #img = Image.open(image)
	#print("image o size ",img.size) #(3000, 4000)

  # Rotate Specific Img for Detection.
  #img = img.rotate(270) #P5-Huraira, P6-HassanIqbal, P7-Zaryab, P8-IbrahimTahir, P9-Anas, P10-Musaab

	# convert to RGB, if needed.
	image = img.convert('RGB')
	#image = ImageOps.grayscale(img)
  
	# convert to array.
	pixels = asarray(image)
 
  # MTCNN Detector arg passed, detect faces in the image.
	results = detector.detect_faces(pixels)
 
	# Drawing boxes for the faces detected & Display.
	pyplot.imshow(pixels)
	ax = pyplot.gca() # getting context for drawing the boxes.
	# plot each box on each face.
	for result in results:
		x, y, w, h = result['box'] # getting coordinates.
		# create the shape
		rect = Rectangle((x, y), w, h, fill=False, color='red')
		# draw the box
		ax.add_patch(rect)
	pyplot.show()

	# Extract the bounding box from the first face so forth all the faces.
	numFaces = len(results) # total faces detected in an image.
	#print(numFaces)
	faces=[]
	#for i in range(numFaces): # Save the locality of each face in list faces.
	x1, y1,w, h = results[0]['box'] # for the first face detected in the Pic.
  # bug fix - incase values are in negative.
	x1, y1 = abs(x1), abs(y1)
	x2, y2 = x1 + w, y1 + h
	# extract the face
	face = pixels[y1:y2, x1:x2]

	# resize pixels to the model size.
	#print("image f1 size ",image.size) #(3000, 4000)
	#image = Image.fromarray(face)
	#print("image f2 size ",image.size)
	image = image.resize(required_size) # working on a single face.
	face_array = np.array(image)
	#face_array = asarray(image)
	#print(face_array.shape) #(96, 96, 3)
  #data[i] = face_array.reshape([96*96*3])
	faces.append(face_array) # save in list.
	return faces

"""testing extract_face ()..."""

from mtcnn.mtcnn import MTCNN
# load image from file
imagePath = "/content/gdrive/My Drive/FYP (2020-2021)/DataSets/p1.jpg" # for a SINGLE IMAGE

detector = MTCNN()
result = extract_face(imagePath, detector, required_size=(160, 160))
print("DETECTED FACES FROM THE IMG")
lenface = len(result)
print(lenface)

# Display faces detected!
c=1
for i in range(lenface):
  pyplot.subplot(4, 6, c) # rows, cols.
  plt.imshow(result[i])
  c+=1
plt.show()
  #cv2_imshow(result[i])

"""**( 2.2 ) PROCESS DATA FUNC** 

*   Calls extract face.
*   Put all the images in one big n-dim array.

"""

def processData(detector ,dir_path):
		count =0
		faces=list()#data = np.ndarray([count,96*96*3])#faces=list()
		for subdir in listdir(dir_path):
				# path
				sub_dir = dir_path + subdir + '/'
				# skip any files that might be in the dir
				if not os.path.isdir(sub_dir):
					continue
				
				for filename in listdir(sub_dir):
						count +=1  
				print("total count: ", count)
				print("TEST FOR: ", subdir)
    
				d_ind=0
				for filename in listdir(sub_dir):
						print("img type: ", filename)
						imagePath = sub_dir + filename
						result = extract_face(imagePath, detector, required_size=(96, 96))
						for i in range(len(result)):
								faces.append( result[i] )#faces.append(result[i]) 
								d_ind+=1
						
        
				#labels = [subdir for _ in range(len(faces))]

    		# summarize progress
				print("Images Loaded for class: ", subdir , " : ", d_ind)
				#print('>loaded %d examples for class: %s' % (len(faces), subdir))
				# store
				#X.extend(faces)
				#Y.extend(labels)
  
		return faces

"""**( 2.3 ) PROCESS TRAIN SET** 


"""

from mtcnn.mtcnn import MTCNN
detector = MTCNN() # creating mtcnn detector using opencv default weights.

print("************************************************************************ TRAINING FACES ************************************************************************ ")
# Loading the train-dataset.
train_directory = '/content/gdrive/MyDrive/FYP (2020-2021)/DataSets/TrainTest/Train/'
train_Set = processData(detector ,train_directory)

#train_Set=numpy.array(train_Set)
#print(train_Set.shape) # printing shapes for ver.

print(len(train_Set))
X_train =np.array(train_Set)
print(X_train.shape) # printing shapes for ver.

"""**( 2.4 ) TRAIN LABEL ASSIGN** 


"""

Y_Set = []
for i in range(X_train.shape[0]):
  if i>= 0 and i<60:
    Y_Set.append(0) #laiba
  elif i>= 60 and i < 121:
    Y_Set.append(1) #abdurrafay
  elif i>= 121 and i < 181:
    Y_Set.append(2) #ruhma
  elif i>= 181 and i < 241:
    Y_Set.append(3) #saad

print(len(Y_Set))
Y_train=np.array(Y_Set)
print(Y_train.shape)
print(Y_train[181])

"""**Encode Label to ONE-HOT VECTOR** 


"""

''' THIS STEP IS USEFUL IN THE TRAINING !
There’s one thing we have to be careful about: Keras expects the training targets to be 10-dimensional vectors, since there are 10 nodes
in our Softmax output layer. Right now, our train_labels and test_labels arrays contain single integers representing the class for each image. 
Like,  print(train_labels[0]) # 5
Conveniently, Keras has a utility method that fixes this exact issue: to_categorical. 
It turns our array of class integers into an array of one-hot vectors instead. 
For example, 2 would become [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] (it’s zero-indexed).
'''
# Label Encoding (be careful! run just once!)
from keras.utils.np_utils import to_categorical 

# convert to one-hot-encoding(one hot vectors)
Y_train = to_categorical(Y_train, num_classes = 4)
print(Y_train.shape)

print(Y_train[0])

"""**( 2.5 ) PROCESS TEST SET** 


"""

print("************************************************************************ TESTING FACES ************************************************************************ ")
# Loading the test-dataset.
test_directory = '/content/gdrive/MyDrive/FYP (2020-2021)/DataSets/TrainTest/Test/'
test_Set= processData(detector ,test_directory)

X_Test = np.array(test_Set)
print(X_Test.shape) # printing shapes for ver..

print(X_Test.shape) # printing shapes for ver..

"""**( 2.6 ) TEST LABEL ASSIGN** 


"""

Y_Set = []
for i in range(X_Test.shape[0]):
  if i>= 0 and i<37:
    Y_Set.append(2) #ruhma
  elif i>= 37 and i < 107:
    Y_Set.append(1) #laiba
  elif i>= 107 and i < 245:
    Y_Set.append(1) # abdurRafay
  elif i>= 245 and i < 321:
    Y_Set.append(3) #saad

Y_test=np.array(Y_Set)
print(Y_test.shape)

"""**Encode Label to ONE-HOT VECTOR** 


"""

Y_test = to_categorical(Y_test, num_classes = 4)

print(Y_test.shape)
print(Y_test[0])

"""**( 2.7 ) TRAIN AND VALIDATION SPLIT** 


"""

# Split the train and the validation set for the fitting
# test size is 10%.
# train size is 90%.
from sklearn.model_selection import train_test_split
x_train, x_val, y_train, y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state = 2)
print("x_train shape: ",x_train.shape)
print("x_val shape: ",x_val.shape)
print("y_train shape: ",y_train.shape)
print("y_val shape :",y_val.shape)

"""# **( 3.0 ) DCNN**"""



model = Sequential()

'''
4 types of layers for our CNN: Convolutional, Max Pooling, FC and Softmax.
> Can also pass these layers in an array in the seq constructor. model = Sequential([... layers])

Hyper-parameters:
      #of filters, filter_size, and pool_size are self-explanatory variables that set the hyperparameters for our CNN.
'''


#1. LAYER  CONV1
'''
 Conv2D( #of filters, filter_size, pad, input img shape ) - only need to specify input layer size for first layer then 
 Keras will automatically infer the shapes of inputs for later layers.
 "valid" means no padding. "same" results in padding evenly to the left/right or up/down of the input such that output has the same height/width
dimension as the input. "causal" results in causal (dilated) convolutions
'''
model.add(Conv2D(filters = 32, kernel_size = (7,7),strides=1, padding = 'Same', input_shape=(96, 96, 3))) #(28, 28, 3)))
model.add(BatchNormalization())
model.add(Activation("relu"))

# POOL layer - POOL1
model.add(MaxPool2D(pool_size=(2, 2)))

#2. LAYER - Conv layer  CONV2
model.add(Conv2D(filters = 64, kernel_size = (5,5), strides=1,padding = 'Same'))
model.add(BatchNormalization())
model.add(Activation("relu"))

# POOL layer - POOL2
model.add(MaxPool2D(pool_size=(2, 2)))

#3. LAYER - Conv layer  CONV3
model.add(Conv2D(filters = 128, kernel_size = (5,5),strides=1, padding = 'Same'))
model.add(BatchNormalization())
model.add(Activation("relu"))

# POOL layer - POOL3
model.add(MaxPool2D(pool_size=(2, 2)))

#FULLY CONNECTED LAYER FC1: fully connected layer with 128 inputs, 512 outputs
model.add(Flatten())
model.add(Dense(512))
model.add(BatchNormalization())
model.add(Activation("relu"))
model.add(Dropout(0.25))

#FULLY CONNECTED LAYER  FC2: fully connected layer with 512 inputs, 512 outputs

#model.add(Flatten())
model.add(Dense(512))
model.add(BatchNormalization())
model.add(Activation("relu"))
model.add(Dropout(0.25))

#OUTPUT LAYER - SOFTMAX: softmax layer for classification: 512 inputs, 4 outputs.
model.add(Dense(4, activation='softmax')) #The output Softmax layer has 10 nodes, one for each class.

model.summary()

optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)

# Compile the model
model.compile(optimizer = optimizer, loss = "categorical_crossentropy", metrics=["accuracy"])

epochs = 10 # for better result increase the epochs. epochs = # of iterations over the entire dataset.
batch_size = 30

# Data Augmentation
datagen = ImageDataGenerator(
        featurewise_center=False,  # set input mean to 0 over the dataset
        samplewise_center=False,  # set each sample mean to 0
        featurewise_std_normalization=False,  # divide inputs by std of the dataset
        samplewise_std_normalization=False,  # divide each input by its std
        zca_whitening=False,  # dimesion reduction
        rotation_range=0.1,  # randomly rotate images in the range
        zoom_range = 0.1, # Randomly zoom image
        width_shift_range=0.1,  # randomly shift images horizontally
        height_shift_range=0.1,  # randomly shift images vertically
        horizontal_flip=False,  # randomly flip images
        vertical_flip=False )  # randomly flip images

datagen.fit(X_train)

history = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),
                              shuffle=True, #veriler random gelip eğitilir
                              epochs=epochs, validation_data = (x_val, y_val),
                              verbose = 2, steps_per_epoch=x_train.shape[0] // batch_size)
#                               callbacks=[checkpointer]) #we save the best weights with checkpointer

loss, acc = model.evaluate(x_train, y_train, verbose=1)
print("  Training : loss %.3f - acc %.3f" % (loss, acc))

print('Cross-validation data:', flush=True)
loss, acc = model.evaluate(x_val, y_val, verbose=1)

print("  Cross-val: loss %.3f - acc %.3f" % (loss, acc))

model.pop()
print(len(model.layers))  # 2

import keras
extract = Model(model.inputs, model.layers[-3]) # Dense(128,...)
features = extract.predict(x_train)

model.save_weights('dcnn1.h5')

"""# **( 4.0 ) ACCURACY TEST SET** 

Predictions
"""

score = model.evaluate(X_Test,Y_test,verbose=0)
print("Test Loss:",score[0])
print("Test Accuracy:",score[1])

print(X_Test.shape[0])

print((score[1]/321)*100)



















