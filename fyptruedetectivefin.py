# -*- coding: utf-8 -*-
"""FypTrueDetectiveFin.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18SbMvqspY8b3zx1njizhKY2izBCZtq6n
"""

# google colab. 

from google.colab import drive
drive.mount('/content/gdrive/')

"""# **( 1.0 ) NECESSARY LIBRARIES & INSTALLATIONS**"""

#Necessary Installations

!pip install --upgrade tf_slim #*

!sudo pip install mtcnn

# confirm mtcnn was installed correctly
import mtcnn
# print version
print(mtcnn.__version__)

# Commented out IPython magic to ensure Python compatibility.
# Necessary Libraries.

from PIL import Image, ImageOps 
from matplotlib import pyplot
from google.colab.patches import cv2_imshow
from matplotlib.patches import Rectangle
import os.path 
from os import listdir
from keras.models import load_model
import numpy as np 
import tensorflow as tf
from numpy import asarray
import matplotlib as mp
# %matplotlib inline
import matplotlib.pyplot as plt
import skimage
import scipy
import tf_slim as slim # tF-Slim is a library that makes defining, training and evaluating neural networks simple:
#from tensorflow.python.compiler.tensorrt import trt_convert as trt
import math
from PIL import Image
from scipy.ndimage import rotate
from os import mkdir
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import seaborn as sns
from scipy import stats
from keras.utils.np_utils import to_categorical 

import warnings
warnings.filterwarnings('ignore')
from sklearn.metrics import confusion_matrix
from keras.models import Sequential, model_from_json
from keras.layers import Dense, Conv2D, Activation, MaxPool2D, Flatten, Dropout, BatchNormalization
from keras.optimizers import RMSprop,Adam
from keras.preprocessing import image
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ModelCheckpoint

"""# **( 2.0 ) FACE DETECTION USING MTCNN**

**( 2.1 ) EXTRACT FACE FUNC** - , 



> **PREPROCESSIGN DATA**


*   Process a single frame
*   Extracting Faces From a single Frame
*   Resize & Bound boxes arround it
"""

# extract a single face from a given photograph

def extract_face(filename, detector,subdir,f_type, required_size=(96, 96)):
  # load image from file.
	image = Image.open(filename) #img = Image.open(image)
	#print("image o size ",img.size) #(3000, 4000)

  # Rotate Specific Img for Detection.
	if (subdir == "P5-Huraira" or subdir == "P6-HassanIqbal" or subdir == "P7-Zaryab" or subdir == "P8-IbrahimTahir" or subdir == "P9-Anas" or subdir == "P10-Musaab" ):
		image = image.rotate(270)

	# convert to array.
	pixels = asarray(image)
 
  # MTCNN Detector arg passed, detect faces in the image.
	results = detector.detect_faces(pixels)
 
	# Drawing boxes for the faces detected & Display.
	pyplot.imshow(pixels)
	ax = pyplot.gca() # getting context for drawing the boxes.
	# plot each box on each face.
	for result in results:
		x, y, w, h = result['box'] # getting coordinates.
		# create the shape
		rect = Rectangle((x, y), w, h, fill=False, color='red')
		# draw the box
		ax.add_patch(rect)
	pyplot.show()

	# Extract the bounding box from the first face so forth all the faces.
	numFaces = len(results) # total faces detected in an image.
	#print(numFaces)
	faces=[]
	#for i in range(numFaces): # Save the locality of each face in list faces.
	x1, y1,w, h = results[0]['box'] # for the first face detected in the Pic. We dont want multiple faces when we knw one face one img lol.
  # bug fixing - incase values are in the -ive.
	x1, y1 = abs(x1), abs(y1)
	x2, y2 = x1 + w, y1 + h
	# extract the face
	face = pixels[y1:y2, x1:x2]

	# resize pixels to the model size.
	#print("image f1 size ",image.size) #(3000, 4000)
	image = Image.fromarray(face)
	#print("image f2 size ",image.size)
	image = ImageOps.grayscale(image) #Grayscale conversion.
  #crop
	if (f_type == "unal"): #only work for unaligned images.
		image = image.crop((2, 4.0, 95, 85)) #(left, top, right, bottom)
	image = image.resize(required_size) # working on a single face.
	face_array = np.array(image)
	#face_array = asarray(image)
	#print(face_array.shape) #(96, 96, 3)
	faces.append(face_array) # save in list.
	return faces

"""testing extract_face ()..."""

from mtcnn.mtcnn import MTCNN
# load image from file
imagePath = "/content/gdrive/My Drive/FYP (2020-2021)/DataSets/p1.jpg" # for a SINGLE IMAGE

detector = MTCNN()
result = extract_face(imagePath, detector,"none","none", required_size=(96, 96))
print("DETECTED FACES FROM THE IMG")
lenface = len(result)
print(lenface)

# Display faces detected!
c=1
for i in range(lenface):
  pyplot.subplot(4, 6, c) # rows, cols.
  plt.imshow(result[i])
  c+=1
plt.show()
  #cv2_imshow(result[i])

# Save ndarray as jprg img.
im = Image.fromarray( result[0])
im.save("your_file.jpeg")

"""testing Crop For Un-aligned Images..."""

# Display faces detected!
c=1
for i in range(lenface):
  pyplot.subplot(4, 6, c) # rows, cols.
  plt.imshow(result[i])
  c+=1
plt.show()
  #cv2_imshow(result[i])

  
# Opens a image in RGB mode 
im = Image.open(r"/content/your_file.jpeg")  
  
# Setting the points for cropped image 
left = 2
top = 5.0
right = 95
bottom = 85
  
# Cropped image of above dimension 
# (It will not change orginal image) 
im1 = im.crop((left, top, right, bottom)) 

# Display faces detected!
c=1
for i in range(lenface):
  pyplot.subplot(4, 6, c) # rows, cols.
  plt.imshow(im1)
  c+=1
plt.show()
  #cv2_imshow(result[i])

"""**( 2.2 ) PROCESS DATA FUNC** 

*   Calls extract face.
*   Put all the images in one big n-dim array.

"""

def processData(detector ,dir_path, f_type):
		count =0
		faces=list()#data = np.ndarray([count,96*96*3])#faces=list()
		subdir_l=[]
		for subdir in listdir(dir_path):
				# path
				sub_dir = dir_path + subdir + '/'
				subdir_l.append(subdir)
				# skipping if there is any file that might be in the dir ex 'data info' file.
				if not os.path.isdir(sub_dir):
					continue
				
				for filename in listdir(sub_dir):
						count +=1  
				print("total count: ", count)
				subdir_l.append(count)
				print("TEST FOR: ", subdir)
    
				d_ind=0
				for filename in listdir(sub_dir):
						print("img type: ", filename)
						imagePath = sub_dir + filename
						result = extract_face(imagePath, detector, subdir, f_type, required_size=(96, 96))
						for i in range(len(result)):
								faces.append( result[i] )#faces.append(result[i]) 
								d_ind+=1
						
        
				#labels = [subdir for _ in range(len(faces))]

    		# summarize progress
				print("Images Loaded for class: ", subdir , " : ", d_ind)
				#print('>loaded %d examples for class: %s' % (len(faces), subdir))
				# store
				#X.extend(faces)
				#Y.extend(labels)
  
		return faces, subdir_l

"""**( 2.3 ) PROCESS TRAIN SET**

*******************


> ***( A ) ALIGNED-FACES***


*******************
"""

from mtcnn.mtcnn import MTCNN
detector = MTCNN() # creating mtcnn detector using opencv default weights.

print("************************************************************************ TRAINING FACES ************************************************************************ ")
# Loading the train-dataset.
train_directory = '/content/gdrive/MyDrive/FYP (2020-2021)/DataSets/TrueDetective-1/Train/'
train_Set, subdir_list = processData(detector ,train_directory, "none")

#train_Set=numpy.array(train_Set)
#print(train_Set.shape) # printing shapes for ver.

print(subdir_list)

print(len(train_Set))
X_train =np.array(train_Set)
print(X_train.shape) # printing shapes for ver.

"""**( 2.4.A ) TRAIN LABEL ASSIGN** 


"""

Y_Set = []
for i in range(692): #X_train.shape[0]
  if i>= 0 and i<48:
    Y_Set.append(0) #hassan
  elif i>= 48 and i < 95:
    Y_Set.append(1) #musaab
  elif i>= 95 and i < 146:
    Y_Set.append(2) #zaryab
  elif i>= 146 and i < 230:
    Y_Set.append(3) #saad
  elif i>= 230 and i < 318:
    Y_Set.append(4) #ibrahim
  elif i>= 318 and i < 374:
    Y_Set.append(5) #ruhma
  elif i>= 374 and i < 480:
    Y_Set.append(6) #huraira
  elif i>= 480 and i < 590:
    Y_Set.append(7) #abdurrafay
  elif i>= 590 and i < 618:
    Y_Set.append(8) #anas
  elif i>= 618 and i < 692:
    Y_Set.append(9) #laiba.

print(len(Y_Set))
Y_train=np.array(Y_Set)
print(Y_train.shape)
print(Y_train[181])

"""**Encode Label to ONE-HOT VECTOR** 


"""

''' THIS STEP IS USEFUL IN THE TRAINING !
There’s one thing we have to be careful about: Keras expects the training targets to be 10-dimensional vectors, since there are 10 nodes
in our Softmax output layer. Right now, our train_labels and test_labels arrays contain single integers representing the class for each image. 
Like,  print(train_labels[0]) # 5
Conveniently, Keras has a utility method that fixes this exact issue: to_categorical. 
It turns our array of class integers into an array of one-hot vectors instead. 
For example, 2 would become [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] (it’s zero-indexed).
'''
# Label Encoding (be careful! run just once!)
from keras.utils.np_utils import to_categorical 

# convert to one-hot-encoding(one hot vectors)
Y_train = to_categorical(Y_train, num_classes = 10)
print(Y_train.shape)

print(Y_train[0])

"""*******************


> ***( B ) UN-ALIGNED-FACES***


*******************
"""

from mtcnn.mtcnn import MTCNN
detector = MTCNN() # creating mtcnn detector using opencv default weights.

print("************************************************************************ TRAINING FACES ************************************************************************ ")
# Loading the train-dataset.
train_directory = '/content/gdrive/MyDrive/FYP (2020-2021)/DataSets/TrueDetective-1/Train/'
train_Set2, subdir_list3 = processData(detector ,train_directory, "unal") #unal = unaligned.

#train_Set2=numpy.array(train_Set2)
#print(train_Set2.shape) # printing shapes for ver.

print(subdir_list3)

print(len(train_Set2))

X_train2 =np.array(train_Set2)
print(X_train2.shape) # printing shapes for ver.

"""**( 2.4.B ) TRAIN LABEL ASSIGN** """

Y_Set2 = []
for i in range(X_train2.shape[0]):
  if i>= 0 and i<48:
    Y_Set2.append(0) #hassan
  elif i>= 48 and i < 95:
    Y_Set2.append(1) #musaab
  elif i>= 95 and i < 146:
    Y_Set2.append(2) #zaryab
  elif i>= 146 and i < 230:
    Y_Set2.append(3) #saad
  elif i>= 230 and i < 318:
    Y_Set2.append(4) #ibrahim
  elif i>= 318 and i < 374:
    Y_Set2.append(5) #ruhma
  elif i>= 374 and i < 480:
    Y_Set2.append(6) #huraira
  elif i>= 480 and i < 590:
    Y_Set2.append(7) #abdurrafay
  elif i>= 590 and i < 618:
    Y_Set2.append(8) #anas
  elif i>= 618 and i < 692:
    Y_Set2.append(9) #laiba.

print(len(Y_Set2))
Y_train2=np.array(Y_Set2)
print(Y_train2.shape)
print(Y_train2[181])

"""**Encode Label to ONE-HOT VECTOR** """

# convert to one-hot-encoding(one hot vectors)
Y_train2 = to_categorical(Y_train2, num_classes = 10)
print(Y_train2.shape)
print(Y_train2[0])



"""**( 2.5 ) PROCESS TEST SET** 


"""

print("************************************************************************ TESTING FACES ************************************************************************ ")
# Loading the test-dataset.
test_directory = '/content/gdrive/MyDrive/FYP (2020-2021)/DataSets/TrueDetective-1/Test/'
test_Set, subdir_list2= processData(detector ,test_directory)

X_test = np.array(test_Set)
print(X_test.shape) # printing shapes for ver..

print(subdir_list2)

print(X_test.shape) # printing shapes for ver.. grayscaled!

"""**( 2.6 ) TEST LABEL ASSIGN** 


"""

Y_Set = []
for i in range(X_test.shape[0]):
  if i>= 0 and i<54:
    Y_Set.append(9) #laiba
  elif i>= 54 and i < 107:
    Y_Set.append(3) #saad
  elif i>= 107 and i < 126:
    Y_Set.append(8) # anas
  elif i>= 126 and i < 158:
    Y_Set.append(0) #hassan
  elif i>= 158 and i < 214:
    Y_Set.append(4) #ibrahim
  elif i>= 214 and i < 250:
    Y_Set.append(5) #ruhma
  elif i>= 250 and i < 339:
    Y_Set.append(7) #abdurrafay
  elif i>= 339 and i < 368:
    Y_Set.append(1) #musaab
  elif i>= 368 and i < 409:
    Y_Set.append(2) #zaryab
  elif i>= 409 and i < 497:
    Y_Set.append(6) #huraira

Y_test=np.array(Y_Set)
print(Y_test.shape)

"""**Encode Label to ONE-HOT VECTOR** 


"""

Y_test = to_categorical(Y_test, num_classes = 10)

print(Y_test.shape)
print(Y_test[0])

"""**( 2.7 ) Saving the Extracted Face Dataset From Train(aligned +unaligned faces) and Test** 


"""

#Arrays compressed into a single file for easy uploading next time.
np.savez_compressed('True-Detective-Aligned-faces-dataset.npz', X_train, Y_train, X_test, Y_test)

#for unaligned
np.savez_compressed('True-Detective-un-Aligned-faces-dataset.npz', X_train2, Y_train2)

"""**loading...**"""

#Loading the face dataset.
faceData = np.load('True-Detective-Aligned-faces-dataset.npz')
X_Train, Y_Train, X_Test, Y_Test = faceData ['arr_0'] ,faceData ['arr_1'] ,faceData ['arr_2'] ,faceData ['arr_3']
print('Loaded: Train: ', X_Train.shape, Y_Train.shape, ' Test: ',X_Test.shape, Y_Test.shape)

#for unaligned
#Loading the face dataset.
faceData2 = np.load('True-Detective-un-Aligned-faces-dataset.npz')
X_Train2, Y_Train2 = faceData2 ['arr_0'] ,faceData2 ['arr_1']
print('Loaded: Train2: ', X_Train2.shape, Y_Train2.shape)

"""**( 2.8 ) Normalization**"""

# normalization

print(X_Train.shape)
print(X_Test.shape)
X_Train = X_Train / 255.0
X_Test = X_Test / 255.0
print("X_train shape: ",X_Train.shape)
print("X_test shape: ",X_Test.shape)

# unaligned normalization
print(X_Train2.shape)
X_Train2 = X_Train2 / 255.0
print("X_train shape: ",X_Train2.shape)

"""**Reshape Keras Requirement** 


"""

# Reshaping X train.
X_Train = np.expand_dims(X_Train, axis=3)
X_TRAIN = np.empty((692, 96, 96, 1))

for k in range(X_Train.shape[0]):
    X_TRAIN[k] = X_Train[k]

print("X_train shape: ",X_Train.shape)
print("X_TRAIN shape: ",X_TRAIN.shape)

# Reshaping X train unaligned.
X_Train2 = np.expand_dims(X_Train2, axis=3)
X_TRAIN2 = np.empty((692, 96, 96, 1))

for k in range(X_Train2.shape[0]):
    X_TRAIN2[k] = X_Train2[k]

print("X_train shape: ",X_Train2.shape)
print("X_TRAIN shape: ",X_TRAIN2.shape)

# Reshaping   x test.
X_Test = np.expand_dims(X_Test, axis=3)
X_TEST = np.empty((497, 96, 96, 1))

for k in range(X_Test.shape[0]):
    X_TEST[k] = X_Test[k]

print("X_train shape: ",X_Test.shape)
print("X_TRAIN shape: ",X_TEST.shape)

"""**( 2.7 ) TRAIN AND VALIDATION SPLIT** 


"""

# Split the train and the validation set for the fitting.
# test size is 10%.
# train size is 90%.
from sklearn.model_selection import train_test_split
x_train, x_val, y_train, y_val = train_test_split(X_TRAIN, Y_Train, test_size = 0.1, random_state = 2)
print("x_train shape: ",x_train.shape)
print("x_val shape: ",x_val.shape)
print("y_train shape: ",y_train.shape)
print("y_val shape :",y_val.shape)

# Split the train and the validation set for the UNALIGNED FACES.

x_train2, x_val2, y_train2, y_val2 = train_test_split(X_TRAIN2, Y_Train2, test_size = 0.1, random_state = 2)
print("x_train shape: ",x_train2.shape)
print("x_val shape: ",x_val2.shape)
print("y_train shape: ",y_train2.shape)
print("y_val shape :",y_val2.shape)

"""# **( 3.0 ) DCNN-1**

**Keras Implementation**

 > For Aligned Faces
"""

model = Sequential()

'''
4 types of layers for our CNN: Convolutional, Max Pooling, FC and Softmax.
> Can also pass these layers in an array in the seq constructor. model = Sequential([... layers])

Hyper-parameters:
      #of filters, filter_size, and pool_size are self-explanatory variables that set the hyperparameters for our CNN.
'''


#1. LAYER  CONV1
'''
 Conv2D( #of filters, filter_size, pad, input img shape ) - only need to specify input layer size for first layer then 
 Keras will automatically infer the shapes of inputs for later layers.
 "valid" means no padding. "same" results in padding evenly to the left/right or up/down of the input such that output has the same height/width
dimension as the input. "causal" results in causal (dilated) convolutions
'''
model.add(Conv2D(filters = 32, kernel_size = (7,7),strides=1, padding = 'Same', input_shape=(96, 96, 1))) #(28, 28, 3)))
model.add(BatchNormalization())
model.add(Activation("relu"))

# POOL layer - POOL1
model.add(MaxPool2D(pool_size=(2, 2)))

#2. LAYER - Conv layer  CONV2
model.add(Conv2D(filters = 64, kernel_size = (5,5), strides=1,padding = 'Same'))
model.add(BatchNormalization())
model.add(Activation("relu"))

# POOL layer - POOL2
model.add(MaxPool2D(pool_size=(2, 2)))

#3. LAYER - Conv layer  CONV3
model.add(Conv2D(filters = 128, kernel_size = (5,5),strides=1, padding = 'Same'))
model.add(BatchNormalization())
model.add(Activation("relu"))

# POOL layer - POOL3
model.add(MaxPool2D(pool_size=(2, 2)))

#FULLY CONNECTED LAYER FC1: fully connected layer with 128 inputs, 512 outputs
model.add(Flatten())
model.add(Dense(512))
model.add(BatchNormalization())
model.add(Activation("relu"))
model.add(Dropout(0.25))

#FULLY CONNECTED LAYER  FC2: fully connected layer with 512 inputs, 512 outputs

#model.add(Flatten())
model.add(Dense(512))
model.add(BatchNormalization())
model.add(Activation("relu"))
model.add(Dropout(0.25))

#OUTPUT LAYER - SOFTMAX: softmax layer for classification: 512 inputs, 4 outputs.
model.add(Dense(10, activation='softmax')) #The output Softmax layer has 10 nodes, one for each class.

model.summary()

optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)

# Compile the model
model.compile(optimizer = optimizer, loss = "categorical_crossentropy", metrics=["accuracy"])

epochs = 17 # for better result increase the epochs. epochs = # of iterations over the entire dataset.
batch_size = 25

"""**DATA AUGMENTATION**

 Increase the diversity of data available for training models, without actually collecting new data.
"""

# Data Augmentation
datagen = ImageDataGenerator(
        featurewise_center=False,  # set input mean to 0 over the dataset
        samplewise_center=False,  # set each sample mean to 0
        featurewise_std_normalization=False,  # divide inputs by std of the dataset
        samplewise_std_normalization=False,  # divide each input by its std
        zca_whitening=False,  # dimesion reduction
        rotation_range=0.1,  # randomly rotate images in the range
        zoom_range = 0.1, # Randomly zoom image
        width_shift_range=0.1,  # randomly shift images horizontally
        height_shift_range=0.1,  # randomly shift images vertically
        horizontal_flip=False,  # randomly flip images
        vertical_flip=False )  # randomly flip images

datagen.fit(x_train)

history = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),
                              shuffle=True, #veriler random gelip eğitilir
                              epochs=epochs, validation_data = (x_val, y_val),
                              verbose = 2, steps_per_epoch=x_train.shape[0] // batch_size)
#                               callbacks=[checkpointer]) #we save the best weights with checkpointer

loss, acc = model.evaluate(x_train, y_train, verbose=1)
print("  Training : loss %.3f - acc %.3f" % (loss, acc))

print('Cross-validation data:', flush=True)
loss, acc = model.evaluate(x_val, y_val, verbose=1)

print("  Cross-val: loss %.3f - acc %.3f" % (loss, acc))

"""predictions"""

score = model.evaluate(X_TEST,Y_Test,verbose=0)
print("Test Loss:",score[0])
print("Test Accuracy:",score[1])

score = model.evaluate(X_TEST,Y_Test,verbose=0)
print("Test Loss:",score[0])
print("Test Accuracy:",score[1])

print("PERCENT ACCURACY ACHIEVED ON TEST SET: ",(score[1]*100), " %")

model.save_weights('dcnn1a.h5')

"""features"""

model.pop()
model.pop()
model.pop()
model.pop()

model.summary()

model.build()

Xtrain_feats1 = model.predict(X_TRAIN)
print(Xtrain_feats1.shape)

print(Xtrain_feats1[0])



"""# **( 4.0 ) DCNN-2**

**Keras Implementation**

 > For Un-Aligned Faces
"""

model2 = Sequential()

'''
4 types of layers for our CNN: Convolutional, Max Pooling, FC and Softmax.
> Can also pass these layers in an array in the seq constructor. model = Sequential([... layers])

Hyper-parameters:
      #of filters, filter_size, and pool_size are self-explanatory variables that set the hyperparameters for our CNN.
'''


#1. LAYER  CONV1
'''
 Conv2D( #of filters, filter_size, pad, input img shape ) - only need to specify input layer size for first layer then 
 Keras will automatically infer the shapes of inputs for later layers.
 "valid" means no padding. "same" results in padding evenly to the left/right or up/down of the input such that output has the same height/width
dimension as the input. "causal" results in causal (dilated) convolutions
'''
model2.add(Conv2D(filters = 32, kernel_size = (7,7),strides=1, padding = 'Same', input_shape=(96, 96, 1))) #(28, 28, 3)))
model2.add(BatchNormalization())
model2.add(Activation("relu"))

# POOL layer - POOL1
model2.add(MaxPool2D(pool_size=(2, 2)))

#2. LAYER - Conv layer  CONV2
model2.add(Conv2D(filters = 64, kernel_size = (5,5), strides=1,padding = 'Same'))
model2.add(BatchNormalization())
model2.add(Activation("relu"))

# POOL layer - POOL2
model2.add(MaxPool2D(pool_size=(2, 2)))

#3. LAYER - Conv layer  CONV3
model2.add(Conv2D(filters = 128, kernel_size = (5,5),strides=1, padding = 'Same'))
model2.add(BatchNormalization())
model2.add(Activation("relu"))

# POOL layer - POOL3
model2.add(MaxPool2D(pool_size=(2, 2)))

#FULLY CONNECTED LAYER FC1: fully connected layer with 128 inputs, 512 outputs
model2.add(Flatten())
model2.add(Dense(512))
model2.add(BatchNormalization())
model2.add(Activation("relu"))
model2.add(Dropout(0.25))

#FULLY CONNECTED LAYER  FC2: fully connected layer with 512 inputs, 512 outputs

#model.add(Flatten())
model2.add(Dense(512))
model2.add(BatchNormalization())
model2.add(Activation("relu"))
model2.add(Dropout(0.25))

#OUTPUT LAYER - SOFTMAX: softmax layer for classification: 512 inputs, 4 outputs.
model2.add(Dense(10, activation='softmax')) #The output Softmax layer has 10 nodes, one for each class.

model2.summary()

optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)

# Compile the model
model2.compile(optimizer = optimizer, loss = "categorical_crossentropy", metrics=["accuracy"])

epochs = 17 # for better result increase the epochs. epochs = # of iterations over the entire dataset.
batch_size = 20

"""**DATA AUGMENTATION**

 Increase the diversity of data available for training models, without actually collecting new data.
"""

# Data Augmentation
datagen = ImageDataGenerator(
        featurewise_center=False,  # set input mean to 0 over the dataset
        samplewise_center=False,  # set each sample mean to 0
        featurewise_std_normalization=False,  # divide inputs by std of the dataset
        samplewise_std_normalization=False,  # divide each input by its std
        zca_whitening=False,  # dimesion reduction
        rotation_range=0.1,  # randomly rotate images in the range
        zoom_range = 0.1, # Randomly zoom image
        width_shift_range=0.1,  # randomly shift images horizontally
        height_shift_range=0.1,  # randomly shift images vertically
        horizontal_flip=False,  # randomly flip images
        vertical_flip=False )  # randomly flip images

datagen.fit(x_train2)

history = model2.fit_generator(datagen.flow(x_train2, y_train2, batch_size=batch_size),
                              shuffle=True, #veriler random gelip eğitilir
                              epochs=epochs, validation_data = (x_val2, y_val2),
                              verbose = 2, steps_per_epoch=x_train2.shape[0] // batch_size)
#                               callbacks=[checkpointer]) #we save the best weights with checkpointer

loss, acc = model2.evaluate(x_train2, y_train2, verbose=1)
print("  Training : loss %.3f - acc %.3f" % (loss, acc))

print('Cross-validation data:', flush=True)
loss, acc = model2.evaluate(x_val2, y_val2, verbose=1)

print("  Cross-val: loss %.3f - acc %.3f" % (loss, acc))

"""Predictions"""

score = model2.evaluate(X_TEST,Y_Test,verbose=0)
print("Test Loss:",score[0])
print("Test Accuracy:",score[1])

model2.save_weights('dcnn2.h5')

"""**FEATURE EXTRACT**"""

model2.pop()
model2.pop()
model2.pop()
model2.pop()

model2.build()

model2.summary()

Xtrain_feats2 = model2.predict(X_TRAIN2)
print(Xtrain_feats2.shape)

print(Xtrain_feats2[0])



"""# **( 5.0 ) PCA**

**(5.1) Concatenating Both Aligned and Un-Aligned Features.**
"""

#
np.savez_compressed('TrueDetective-feat1-feat2.npz', Xtrain_feats1, Xtrain_feats2)

Trainfeatures = np.load('TrueDetective-feat1-feat2.npz')
feat1, feat2 = Trainfeatures ['arr_0'] ,Trainfeatures ['arr_1']
print('Loaded: Train features: Aligned', feat1.shape," Un-Aligned: ", feat2.shape)

Train_Feats = np.concatenate([feat1, feat2], 1)

print(Train_Feats.shape)
print(Train_Feats[0])
print(Train_Feats[0].shape)

"""**(5.2) Scree - plot**"""

cm = PCA( n_components = 10)
cm.fit(Train_Feats)

plt.ylabel("eigen values")
plt.xlabel("# of feats")
plt.title("pca eigen values")
plt.ylim(0,max(cm.explained_variance_))
plt.style.context('seaborn-whitegrid')
plt.axhline(y=1, color='r', linestyle='--')
plt.plot(cm.explained_variance_)
plt.show() # just pca 1 & 2 are enuf to describe the data (elbow point)

"""**(5.3) APPLY PCA**"""

from sklearn.decomposition import PCA
k=1 # no of dimensions.
pca = PCA(k) #pca instance created.
print(Train_Feats)

#Reduced
red = pca.fit_transform(Train_Feats)
print(red.shape ) # fit and transform.

# Centering the data points.
Train_Feats = Train_Feats - Train_Feats.mean(axis=0) 
print("Centered Matrix: ", Train_Feats)

# co-variance matrix of our features.
covMat = np.cov(Train_Feats.T) / Train_Feats.shape[0]
print("Covariance matrix: ", covMat)

# Perform Eigen-decomposition on Covariance Matrix.
v, w = np.linalg.eig(covMat)

# using argsort to sort Eigenvectors According to Eigenvalues.
idx = v.argsort()[::-1] # Sort descending and get sorted indices.
v = v[idx] # Use indices on eigv vector.
w = w[:,idx] 

print("Eigenvalue vektoru: ", v)
print("Eigenvektorler: ", w)

#reduced feature space [result of pca].
reducedTrain =Train_Feats.dot(w[:, :k])
print("Sonuc: ", reducedTrain.shape)

"""**(5.4) X-TEST FEATURES**"""

from keras.models import load_model
# load model
Test_Feat1 = model.predict(X_TEST) #X_TEST,Y_Test
Test_Feat2 = model2.predict(X_TEST)

print(Test_Feat1.shape)
print(Test_Feat2.shape)

# save features.
np.savez_compressed('Test-feat1-feat2.npz', Test_Feat1, Test_Feat2)

#load features.
Testfeatures = np.load('Test-feat1-feat2.npz')
Testfeat1, Testfeat2 = Testfeatures ['arr_0'] ,Testfeatures ['arr_1']
print('Loaded: Test features: dcnn1', Testfeat1.shape," dcnn2: ", Testfeat2.shape)

Test_Feats = np.concatenate([Testfeat1, Testfeat2], 1)

print(Test_Feats.shape)

#pca 
k=1 # no of dimensions.
pca = PCA(k) #pca instance created.
#Reduced
red2 = pca.fit_transform(Test_Feats)
print(red2.shape ) # fit and transform.


# Centering the data points.
Test_Feats = Test_Feats - Test_Feats.mean(axis=0) 
print("Centered Matrix: ", Test_Feats)

# co-variance matrix of our features.
covMat = np.cov(Test_Feats.T) / Test_Feats.shape[0]
print("Covariance matrix: ", covMat)

# Perform Eigen-decomposition on Covariance Matrix.
v, w = np.linalg.eig(covMat)

# using argsort to sort Eigenvectors According to Eigenvalues.
idx = v.argsort()[::-1] # Sort descending and get sorted indices.
v = v[idx] # Use indices on eigv vector.
w = w[:,idx] 

print("Eigenvalue vektoru: ", v)
print("Eigenvektorler: ", w)

#reduced feature space [result of pca].
reducedTest =Test_Feats.dot(w[:, :k])
print("Sonuc: ", reducedTest.shape)



"""# **( 6.0 ) SPM** 

Predictions
"""

# save reduced spaces.
np.savez_compressed('Final-Train-Test-Feats.npz', red, red2)

#Load train, test.
FDATA = np.load('Final-Train-Test-Feats.npz')
TRAINSET, TESTSET = FDATA ['arr_0'] ,FDATA ['arr_1']
print('Loaded: DATA: TRAIN: ', TRAINSET.shape," TEST: ", TESTSET.shape)

import sklearn.preprocessing as sk

# normalize input vectors
in_encoder = sk.Normalizer(norm='l2')
TRAIN_DATA = in_encoder.transform(TRAINSET)
TEST_DATA = in_encoder.transform(TESTSET)

# label encode targets
out_encoder = sk.LabelEncoder()
out_encoder.fit(trainy)
trainy = out_encoder.transform(trainy)
testy = out_encoder.transform(testy)

# Y_Set  #TRAIN
TRAIN_LABEL = []
for i in range(len(Y_Set)):
  TRAIN_LABEL.append(Y_Set[i])

print(len(TRAIN_LABEL))

TEST_LABEL = []
for i in range(497):
  if i>= 0 and i<54:
    TEST_LABEL.append(9) #laiba
  elif i>= 54 and i < 107:
    TEST_LABEL.append(3) #saad
  elif i>= 107 and i < 126:
    TEST_LABEL.append(8) # anas
  elif i>= 126 and i < 158:
    TEST_LABEL.append(0) #hassan
  elif i>= 158 and i < 214:
    TEST_LABEL.append(4) #ibrahim
  elif i>= 214 and i < 250:
    TEST_LABEL.append(5) #ruhma
  elif i>= 250 and i < 339:
    TEST_LABEL.append(7) #abdurrafay
  elif i>= 339 and i < 368:
    TEST_LABEL.append(1) #musaab
  elif i>= 368 and i < 409:
    TEST_LABEL.append(2) #zaryab
  elif i>= 409 and i < 497:
    TEST_LABEL.append(6) #huraira


print(len(TEST_LABEL))

from sklearn import svm

# fit model
model = svm.SVC(kernel='linear', probability=True)
model.fit(TRAIN_DATA, TRAIN_LABEL)

from sklearn.metrics import accuracy_score

# predict
yhat_train = model.predict(TRAIN_DATA)
yhat_test = model.predict(TEST_DATA)
# score
score_train = accuracy_score(TRAIN_LABEL, yhat_train)
score_test = accuracy_score(TEST_LABEL, yhat_test)
# summarize
print('Accuracy: train=%.3f, test=%.3f' % (score_train*100, score_test*100))



https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006907

https://dev.to/akaame/implementing-simple-pca-using-numpy-3k0a

"""# **( 4.0 ) ACCURACY TEST SET** 

Predictions
"""

Y_Set







